# ğŸ“ Guia de PadrÃµes e Boas PrÃ¡ticas Git para DataOps

Um guia completo de organizaÃ§Ã£o, estrutura e padrÃµes para projetos de dados usando Git.

## ğŸ¯ Ãndice

1. [Conventional Commits - PadrÃ£o de Mensagens](#conventional-commits)
2. [Estrutura de Branches para DataOps](#estrutura-de-branches)
3. [Estrutura de RepositÃ³rios](#estrutura-de-repositÃ³rios)
4. [Nomenclatura e OrganizaÃ§Ã£o](#nomenclatura)
5. [Versionamento de Dados](#versionamento-de-dados)
6. [.gitignore para DataOps](#gitignore-dataops)
7. [DocumentaÃ§Ã£o ObrigatÃ³ria](#documentaÃ§Ã£o)
8. [Code Review e Qualidade](#code-review)
9. [Workflows DataOps](#workflows-dataops)
10. [SeguranÃ§a e Compliance](#seguranÃ§a)
11. [Checklist de Projeto](#checklist)

---

## ğŸ“ Conventional Commits - PadrÃ£o de Mensagens {#conventional-commits}

### Por Que Usar PadrÃ£o?

âœ… **HistÃ³rico legÃ­vel** - FÃ¡cil entender o que mudou
âœ… **AutomatizaÃ§Ã£o** - Gerar CHANGELOGs automaticamente
âœ… **Semantic Versioning** - Determinar versÃµes (1.2.3)
âœ… **Rastreabilidade** - Encontrar mudanÃ§as especÃ­ficas
âœ… **ComunicaÃ§Ã£o clara** - Time todo entende

### Formato PadrÃ£o

```
<tipo>(<escopo>): <descriÃ§Ã£o curta>

<corpo opcional - detalhes>

<rodapÃ© opcional - breaking changes, issues>
```

### Tipos de Commit

#### Para CÃ³digo/Scripts:

```bash
feat:      Nova funcionalidade
fix:       CorreÃ§Ã£o de bug
refactor:  RefatoraÃ§Ã£o sem mudar comportamento
perf:      Melhoria de performance
style:     FormataÃ§Ã£o, ponto e vÃ­rgula, etc
test:      Adicionar ou corrigir testes
docs:      DocumentaÃ§Ã£o apenas
build:     Sistema de build (npm, pip, etc)
ci:        CI/CD (GitHub Actions, Jenkins)
chore:     ManutenÃ§Ã£o (atualizar deps, configs)
revert:    Reverter commit anterior
```

#### EspecÃ­ficos para DataOps:

```bash
data:      MudanÃ§as em datasets ou schemas
pipeline:  MudanÃ§as em pipelines de dados
model:     MudanÃ§as em modelos (ML, estatÃ­sticos)
schema:    MudanÃ§as em schemas de dados
query:     MudanÃ§as em queries SQL/NoSQL
etl:       MudanÃ§as em processos ETL/ELT
viz:       MudanÃ§as em visualizaÃ§Ãµes/dashboards
config:    MudanÃ§as em configuraÃ§Ãµes de infra
```

### Exemplos PrÃ¡ticos

#### âœ… Bons Exemplos:

```bash
# CÃ³digo
feat(api): adiciona endpoint de autenticaÃ§Ã£o
fix(pipeline): corrige erro de encoding em CSV
refactor(etl): otimiza processo de extraÃ§Ã£o do S3
perf(query): adiciona Ã­ndice na tabela de vendas

# Dados
data(vendas): adiciona dataset de Q4 2025
schema(usuarios): adiciona campo email_verificado
pipeline(vendas): implementa validaÃ§Ã£o de dados
model(churn): atualiza modelo de Random Forest para XGBoost

# DocumentaÃ§Ã£o
docs(readme): adiciona guia de instalaÃ§Ã£o
docs(api): documenta endpoints de relatÃ³rios

# Infraestrutura
ci(github): adiciona workflow de testes
config(airflow): atualiza configuraÃ§Ã£o de memÃ³ria
build(docker): otimiza imagem para produÃ§Ã£o

# Breaking Changes
feat(api)!: remove endpoint legado de clientes

BREAKING CHANGE: O endpoint /api/v1/clients foi removido.
Use /api/v2/customers no lugar.
```

#### âŒ Exemplos Ruins:

```bash
# Muito vago
"update"
"fix"
"changes"
"stuff"

# Sem tipo
"adiciona validaÃ§Ã£o"
"corrige bug"

# Muito longo
"feat: adiciona nova funcionalidade super importante que valida dados e envia email e atualiza dashboard"

# Sem contexto
"fix: bug"
"feat: feature"
```

### Escopos Sugeridos para DataOps

```bash
# Por camada de dados
(bronze)      Dados brutos
(silver)      Dados limpos/transformados
(gold)        Dados analÃ­ticos/agregados

# Por domÃ­nio
(vendas)      DomÃ­nio de vendas
(marketing)   DomÃ­nio de marketing
(financeiro)  DomÃ­nio financeiro
(clientes)    DomÃ­nio de clientes

# Por componente
(airflow)     Apache Airflow
(dbt)         dbt (Data Build Tool)
(spark)       Apache Spark
(snowflake)   Snowflake
(redshift)    Amazon Redshift
(databricks)  Databricks

# Por tipo de ativo
(dashboard)   Dashboards/Viz
(report)      RelatÃ³rios
(alert)       Alertas/Monitoramento
```

### Estrutura Completa de Commit

```bash
# Commit com tudo
git commit -m "feat(pipeline): adiciona validaÃ§Ã£o de qualidade de dados

- Implementa validaÃ§Ã£o de nulos
- Adiciona checagem de tipos de dados
- Cria alertas para dados fora do padrÃ£o
- Integra com Great Expectations

Refs: JIRA-123
Reviewed-by: Maria Silva"
```

### Template de Commit

Crie arquivo `.gitmessage`:

```bash
# <tipo>(<escopo>): <descriÃ§Ã£o curta>
# |<----  MÃ¡ximo 50 caracteres  ---->|

# <corpo - explicaÃ§Ã£o detalhada>
# |<----  MÃ¡ximo 72 caracteres por linha  ---->|

# <rodapÃ©>
# Refs: #issue
# Reviewed-by: Nome
# BREAKING CHANGE: descriÃ§Ã£o

# Tipos disponÃ­veis:
# feat:     Nova funcionalidade
# fix:      CorreÃ§Ã£o de bug
# data:     MudanÃ§as em dados
# pipeline: MudanÃ§as em pipeline
# model:    MudanÃ§as em modelos
# schema:   MudanÃ§as em schema
# docs:     DocumentaÃ§Ã£o
# refactor: RefatoraÃ§Ã£o
# test:     Testes
# ci:       CI/CD
# config:   ConfiguraÃ§Ã£o
```

Configure:
```bash
git config --global commit.template ~/.gitmessage
```

---

## ğŸŒ¿ Estrutura de Branches para DataOps {#estrutura-de-branches}

### GitFlow Adaptado para DataOps

```
production (main)
    â†“
staging (develop)
    â†“
feature/*       Novas funcionalidades/pipelines
    â†“
hotfix/*        CorreÃ§Ãµes urgentes em produÃ§Ã£o
    â†“
data/*          MudanÃ§as em dados/schemas
    â†“
experiment/*    Experimentos/anÃ¡lises temporÃ¡rias
```

### Branches Principais

#### 1. **production** (ou **main**)
```
âœ… CÃ³digo em produÃ§Ã£o
âœ… Sempre estÃ¡vel
âœ… Deploy automÃ¡tico
âœ… Protected branch
âœ… Requer aprovaÃ§Ã£o
```

**Regras:**
- âŒ Nunca commit direto
- âœ… Apenas via Pull Request
- âœ… CI/CD deve passar
- âœ… Code review obrigatÃ³rio
- âœ… Testes 100% passing

#### 2. **staging** (ou **develop**)
```
âœ… Ambiente de staging
âœ… IntegraÃ§Ã£o contÃ­nua
âœ… Testes antes de produÃ§Ã£o
âœ… ValidaÃ§Ã£o de stakeholders
```

**Regras:**
- âœ… Merge de features aqui primeiro
- âœ… Testes de integraÃ§Ã£o
- âœ… ValidaÃ§Ã£o de dados
- âœ… QA review

### Branches de Trabalho

#### **feature/\*** - Novas Features

```bash
# Nomenclatura
feature/nome-da-feature
feature/DAT-123-pipeline-vendas
feature/implementa-modelo-churn

# Exemplos
feature/vendas-pipeline-mensal
feature/DAT-456-dashboard-executivo
feature/integracao-salesforce
```

**Ciclo de vida:**
```bash
# Criar
git checkout staging
git pull
git checkout -b feature/DAT-123-pipeline-vendas

# Desenvolver
git add .
git commit -m "feat(pipeline): implementa extraÃ§Ã£o de vendas"

# Finalizar
git push origin feature/DAT-123-pipeline-vendas
# Abrir PR para staging
# ApÃ³s merge, deletar branch
```

#### **hotfix/\*** - CorreÃ§Ãµes Urgentes

```bash
# Nomenclatura
hotfix/nome-do-bug
hotfix/BUG-789-erro-calculo

# Exemplos
hotfix/corrige-encoding-utf8
hotfix/BUG-789-erro-agregacao-vendas
```

**Ciclo:**
```bash
# Criar DIRETO da production
git checkout production
git pull
git checkout -b hotfix/BUG-789-erro-agregacao

# Corrigir
git commit -m "fix(vendas): corrige cÃ¡lculo de agregaÃ§Ã£o"

# Merge em production E staging
git checkout production
git merge hotfix/BUG-789-erro-agregacao
git checkout staging
git merge hotfix/BUG-789-erro-agregacao
```

#### **data/\*** - MudanÃ§as em Dados

```bash
# Nomenclatura
data/nome-da-mudanca
data/adiciona-dataset-q4
data/schema-v2-usuarios

# Exemplos
data/adiciona-vendas-2025
data/migracao-schema-usuarios-v2
data/corrige-tipos-coluna-data
```

**Uso:**
```bash
# Para mudanÃ§as em schemas, seeds, fixtures
git checkout -b data/schema-v2-clientes

# Commits especÃ­ficos
git commit -m "schema(clientes): adiciona campo cpf_validado"
git commit -m "data(clientes): adiciona dataset de teste"
```

#### **experiment/\*** - Experimentos

```bash
# Nomenclatura
experiment/nome-experimento
experiment/EXP-123-teste-modelo-xgboost

# Exemplos
experiment/teste-feature-engineering
experiment/analise-outliers-vendas
experiment/poc-dbt-incremental
```

**CaracterÃ­sticas:**
- â° TemporÃ¡rio
- ğŸ§ª ExploratÃ³rio
- ğŸ—‘ï¸ Pode ser deletado depois
- ğŸ“Š Para notebooks/anÃ¡lises

#### **model/\*** - Modelos ML

```bash
# Nomenclatura
model/nome-do-modelo
model/churn-v2
model/recomendacao-produtos

# Exemplos
model/churn-random-forest-v3
model/segmentacao-clientes-kmeans
model/previsao-demanda-prophet
```

### Nomenclatura de Branches

#### âœ… Boas PrÃ¡ticas:

```bash
# Formato
<tipo>/<descriÃ§Ã£o-kebab-case>
<tipo>/<ticket>-<descriÃ§Ã£o>

# Exemplos
feature/pipeline-vendas-diario
feature/DAT-123-etl-vendas
hotfix/corrige-timezone-logs
data/schema-v2-produtos
experiment/teste-apache-iceberg
model/churn-xgboost-v2
```

#### âŒ Evite:

```bash
# Muito genÃ©rico
feature/update
feature/fix

# Com espaÃ§os ou caracteres especiais
feature/Pipeline de Vendas
feature/vendas_pipeline

# Muito longo
feature/implementa-novo-pipeline-de-vendas-com-validacao-e-alertas

# Sem contexto
feature/123
feature/test
```

### ProteÃ§Ã£o de Branches

Configure no GitHub/GitLab:

```yaml
# production
protections:
  - require_pull_request: true
  - require_reviews: 2
  - require_status_checks: true
  - require_up_to_date: true
  - no_force_push: true
  - no_delete: true

# staging
protections:
  - require_pull_request: true
  - require_reviews: 1
  - require_status_checks: true
```

---

## ğŸ“ Estrutura de RepositÃ³rios {#estrutura-de-repositÃ³rios}

### Mono-repo vs Multi-repo

#### **Mono-repo** (Recomendado para DataOps)

```
data-platform/
â”œâ”€â”€ pipelines/          Todos os pipelines
â”œâ”€â”€ models/             Todos os modelos
â”œâ”€â”€ transformations/    TransformaÃ§Ãµes dbt
â”œâ”€â”€ config/             ConfiguraÃ§Ãµes
â””â”€â”€ docs/               DocumentaÃ§Ã£o
```

**Vantagens:**
- âœ… Versionamento unificado
- âœ… RefatoraÃ§Ã£o fÃ¡cil
- âœ… DependÃªncias claras
- âœ… CI/CD simplificado

#### **Multi-repo** (Para times grandes)

```
vendas-pipeline/        RepositÃ³rio de vendas
marketing-pipeline/     RepositÃ³rio de marketing
ml-models/             RepositÃ³rio de modelos
data-quality/          RepositÃ³rio de qualidade
```

**Vantagens:**
- âœ… Autonomia de times
- âœ… Deploys independentes
- âœ… Controle de acesso granular

### Estrutura PadrÃ£o de Projeto DataOps

```
meu-projeto-data/
â”‚
â”œâ”€â”€ .github/                          # GitHub configs
â”‚   â”œâ”€â”€ workflows/                    # GitHub Actions
â”‚   â”‚   â”œâ”€â”€ ci.yml                   # CI pipeline
â”‚   â”‚   â”œâ”€â”€ cd-staging.yml           # Deploy staging
â”‚   â”‚   â””â”€â”€ cd-production.yml        # Deploy prod
â”‚   â”œâ”€â”€ PULL_REQUEST_TEMPLATE.md     # Template de PR
â”‚   â””â”€â”€ ISSUE_TEMPLATE/              # Templates de issues
â”‚
â”œâ”€â”€ airflow/                          # Apache Airflow
â”‚   â”œâ”€â”€ dags/                        # DAGs
â”‚   â”‚   â”œâ”€â”€ vendas_diario.py
â”‚   â”‚   â”œâ”€â”€ etl_clientes.py
â”‚   â”‚   â””â”€â”€ ml_pipeline.py
â”‚   â”œâ”€â”€ plugins/                     # Plugins customizados
â”‚   â”œâ”€â”€ config/                      # ConfiguraÃ§Ãµes
â”‚   â””â”€â”€ tests/                       # Testes de DAGs
â”‚
â”œâ”€â”€ dbt/                              # dbt (Data Build Tool)
â”‚   â”œâ”€â”€ models/                      # Modelos SQL
â”‚   â”‚   â”œâ”€â”€ staging/                # Camada bronze/staging
â”‚   â”‚   â”œâ”€â”€ intermediate/           # Camada silver
â”‚   â”‚   â””â”€â”€ marts/                  # Camada gold
â”‚   â”œâ”€â”€ macros/                      # Macros reutilizÃ¡veis
â”‚   â”œâ”€â”€ tests/                       # Testes de dados
â”‚   â”œâ”€â”€ seeds/                       # Dados estÃ¡ticos (CSV)
â”‚   â”œâ”€â”€ snapshots/                   # Snapshots SCD
â”‚   â””â”€â”€ dbt_project.yml             # ConfiguraÃ§Ã£o
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter Notebooks
â”‚   â”œâ”€â”€ exploratory/                 # AnÃ¡lises exploratÃ³rias
â”‚   â”œâ”€â”€ experiments/                 # Experimentos
â”‚   â””â”€â”€ reports/                     # RelatÃ³rios
â”‚
â”œâ”€â”€ src/                              # CÃ³digo fonte
â”‚   â”œâ”€â”€ pipelines/                   # Pipelines ETL
â”‚   â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â””â”€â”€ load/
â”‚   â”œâ”€â”€ models/                      # Modelos ML
â”‚   â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â””â”€â”€ evaluation/
â”‚   â”œâ”€â”€ utils/                       # UtilitÃ¡rios
â”‚   â””â”€â”€ config/                      # Configs em cÃ³digo
â”‚
â”œâ”€â”€ tests/                            # Testes
â”‚   â”œâ”€â”€ unit/                        # Testes unitÃ¡rios
â”‚   â”œâ”€â”€ integration/                 # Testes integraÃ§Ã£o
â”‚   â””â”€â”€ fixtures/                    # Dados de teste
â”‚
â”œâ”€â”€ data/                             # Dados (NÃƒO commitar grandes arquivos)
â”‚   â”œâ”€â”€ raw/                         # Dados brutos (samples)
â”‚   â”œâ”€â”€ processed/                   # Dados processados
â”‚   â””â”€â”€ seeds/                       # Dados estÃ¡ticos pequenos
â”‚
â”œâ”€â”€ sql/                              # Queries SQL
â”‚   â”œâ”€â”€ ddl/                         # CREATE, ALTER, DROP
â”‚   â”œâ”€â”€ dml/                         # INSERT, UPDATE, DELETE
â”‚   â””â”€â”€ queries/                     # SELECT queries
â”‚
â”œâ”€â”€ config/                           # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ dev.yml                      # Ambiente dev
â”‚   â”œâ”€â”€ staging.yml                  # Ambiente staging
â”‚   â”œâ”€â”€ production.yml               # Ambiente prod
â”‚   â””â”€â”€ schema.yml                   # Schemas de dados
â”‚
â”œâ”€â”€ docs/                             # DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ architecture/                # Arquitetura
â”‚   â”œâ”€â”€ runbooks/                    # Runbooks operacionais
â”‚   â”œâ”€â”€ data_catalog/                # CatÃ¡logo de dados
â”‚   â””â”€â”€ decisions/                   # ADRs (Architecture Decision Records)
â”‚
â”œâ”€â”€ scripts/                          # Scripts auxiliares
â”‚   â”œâ”€â”€ setup/                       # Setup ambiente
â”‚   â”œâ”€â”€ migration/                   # MigraÃ§Ãµes de dados
â”‚   â””â”€â”€ utilities/                   # UtilitÃ¡rios
â”‚
â”œâ”€â”€ docker/                           # Docker
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ .dockerignore
â”‚
â”œâ”€â”€ .gitignore                        # Git ignore
â”œâ”€â”€ .env.example                      # Exemplo de .env
â”œâ”€â”€ README.md                         # DocumentaÃ§Ã£o principal
â”œâ”€â”€ CONTRIBUTING.md                   # Guia de contribuiÃ§Ã£o
â”œâ”€â”€ CHANGELOG.md                      # HistÃ³rico de mudanÃ§as
â”œâ”€â”€ requirements.txt                  # DependÃªncias Python
â”œâ”€â”€ pyproject.toml                    # Config Python (alternativa)
â”œâ”€â”€ Makefile                          # Comandos Ãºteis
â””â”€â”€ VERSION                           # VersÃ£o do projeto
```

### Arquivos Essenciais

#### **README.md**

```markdown
# Nome do Projeto

## ğŸ“Š DescriÃ§Ã£o
Pipeline de dados de vendas que extrai, transforma e carrega dados...

## ğŸ—ï¸ Arquitetura
[Diagrama da arquitetura]

## ğŸš€ Quick Start
\`\`\`bash
# Clone
git clone https://github.com/org/projeto.git

# Setup
make setup

# Run
make run
\`\`\`

## ğŸ“ Estrutura
- `/airflow` - DAGs do Airflow
- `/dbt` - Modelos dbt
- `/src` - CÃ³digo fonte

## ğŸ§ª Testes
\`\`\`bash
make test
\`\`\`

## ğŸš¢ Deploy
Ver [DEPLOY.md](docs/DEPLOY.md)

## ğŸ“– DocumentaÃ§Ã£o
Ver [docs/](docs/)

## ğŸ‘¥ Time
- Data Engineer: JoÃ£o Silva
- Data Analyst: Maria Santos
```

#### **CONTRIBUTING.md**

```markdown
# Guia de ContribuiÃ§Ã£o

## Workflow
1. Crie branch: `git checkout -b feature/sua-feature`
2. Comite: `git commit -m "feat: descriÃ§Ã£o"`
3. Push: `git push origin feature/sua-feature`
4. Abra Pull Request

## PadrÃµes
- Conventional Commits
- Code review obrigatÃ³rio
- Testes devem passar
- Documentar mudanÃ§as

## Code Review
- [ ] CÃ³digo limpo e legÃ­vel
- [ ] Testes incluÃ­dos
- [ ] DocumentaÃ§Ã£o atualizada
- [ ] CI passa
```

#### **CHANGELOG.md**

```markdown
# Changelog

## [2.1.0] - 2026-01-13
### Added
- Pipeline de vendas diÃ¡rio
- Dashboard executivo

### Fixed
- CorreÃ§Ã£o de timezone em logs

### Changed
- AtualizaÃ§Ã£o do modelo de churn para XGBoost

## [2.0.0] - 2025-12-01
### Added
- MigraÃ§Ã£o para dbt
- ImplementaÃ§Ã£o de data quality

### Breaking Changes
- Schema de clientes alterado
```

---

## ğŸ·ï¸ Nomenclatura e OrganizaÃ§Ã£o {#nomenclatura}

### Arquivos

#### âœ… Boas PrÃ¡ticas:

```python
# Lowercase com underscores (snake_case)
pipeline_vendas_diario.py
extract_salesforce.py
transform_clientes.py
model_churn_v2.py

# SQL
dim_clientes.sql
fct_vendas.sql
stg_erp_pedidos.sql

# Notebooks (data + descriÃ§Ã£o)
2026_01_13_analise_churn.ipynb
2025_12_exploracao_vendas_q4.ipynb

# Config
config_dev.yml
schema_vendas.json
```

#### âŒ Evite:

```python
# CamelCase ou mixed
PipelineVendas.py
Extract-Salesforce.py

# Nomes genÃ©ricos
pipeline.py
script.py
analysis.ipynb

# EspaÃ§os ou caracteres especiais
pipeline vendas.py
anÃ¡lise_vendas.py  # evite acentos
```

### VariÃ¡veis e FunÃ§Ãµes

```python
# Python: snake_case
def extract_salesforce_data():
    client_name = "Acme Corp"
    total_revenue = 1000000
    
# SQL: lowercase
SELECT
    client_id,
    client_name,
    total_revenue
FROM dim_clients

# Constantes: UPPER_CASE
DATABASE_URL = "postgresql://..."
MAX_RETRIES = 3
DEFAULT_BATCH_SIZE = 1000
```

### Tabelas e Schemas

```sql
-- Prefixos por camada
-- Bronze/Raw: raw_
raw_salesforce_accounts
raw_erp_orders

-- Silver/Staging: stg_
stg_salesforce_accounts
stg_erp_orders

-- Gold/Marts: dim_ (dimensÃµes), fct_ (fatos)
dim_clients
dim_products
fct_sales
fct_orders

-- AgregaÃ§Ãµes: agg_
agg_sales_daily
agg_revenue_monthly

-- Snapshots: snap_
snap_clients_scd2

-- Views: v_ ou vw_
v_sales_summary
vw_active_clients
```

### Colunas

```sql
-- PadrÃ£o: lowercase com underscore
client_id
client_name
created_at
updated_at

-- IDs: <tabela>_id
client_id
product_id
order_id

-- Datas: <evento>_at ou <evento>_date
created_at
updated_at
deleted_at
order_date
birth_date

-- Booleanos: is_ ou has_
is_active
is_deleted
has_subscription
has_children

-- Valores monetÃ¡rios: especificar moeda
revenue_usd
price_brl
cost_eur
```

---

## ğŸ“¦ Versionamento de Dados {#versionamento-de-dados}

### Semantic Versioning para Dados

```
MAJOR.MINOR.PATCH
  2  . 1  .  3

MAJOR: Breaking changes (schema incompatÃ­vel)
MINOR: Novos campos/tabelas (compatÃ­vel)
PATCH: CorreÃ§Ãµes de dados
```

### Exemplos:

```sql
-- v1.0.0 - Schema inicial
CREATE TABLE clients (
    id INTEGER PRIMARY KEY,
    name VARCHAR(100)
);

-- v1.1.0 - Adiciona campo (compatÃ­vel)
ALTER TABLE clients ADD COLUMN email VARCHAR(100);

-- v2.0.0 - Breaking change (renomeia campo)
ALTER TABLE clients RENAME COLUMN name TO full_name;

-- v2.0.1 - CorreÃ§Ã£o de dados
UPDATE clients SET email = LOWER(email);
```

### Versionamento de Modelos ML

```
models/
â”œâ”€â”€ churn/
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”œâ”€â”€ model.pkl
â”‚   â”‚   â”œâ”€â”€ metadata.json
â”‚   â”‚   â””â”€â”€ metrics.json
â”‚   â”œâ”€â”€ v2/
â”‚   â”‚   â”œâ”€â”€ model.pkl
â”‚   â”‚   â”œâ”€â”€ metadata.json
â”‚   â”‚   â””â”€â”€ metrics.json
â”‚   â””â”€â”€ CHANGELOG.md
```

```json
// metadata.json
{
  "version": "2.1.0",
  "created_at": "2026-01-13",
  "algorithm": "XGBoost",
  "features": ["idade", "renda", "tempo_cliente"],
  "performance": {
    "accuracy": 0.92,
    "precision": 0.89,
    "recall": 0.91
  },
  "training_data": {
    "start_date": "2024-01-01",
    "end_date": "2025-12-31",
    "n_samples": 100000
  }
}
```

### Tags Git para VersÃµes

```bash
# Criar tag de versÃ£o
git tag -a v2.1.0 -m "Release 2.1.0 - Novo pipeline de vendas"
git push origin v2.1.0

# Listar versÃµes
git tag -l

# Checkout de versÃ£o especÃ­fica
git checkout v2.0.0
```

### Versionamento em Arquivos

```python
# No cÃ³digo
__version__ = "2.1.0"

# No nome do arquivo (quando necessÃ¡rio)
schema_clientes_v2.json
modelo_churn_v3.pkl

# Em metadados
"""
Pipeline de Vendas
Version: 2.1.0
Last Updated: 2026-01-13
Author: JoÃ£o Silva
"""
```

---

## ğŸš« .gitignore para DataOps {#gitignore-dataops}

### Template Completo

```gitignore
# ============================================
# Python
# ============================================
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/
.venv
pip-log.txt
pip-delete-this-directory.txt
.pytest_cache/
.coverage
htmlcov/
*.egg
*.egg-info/
dist/
build/
*.whl

# ============================================
# Jupyter Notebooks
# ============================================
.ipynb_checkpoints/
*/.ipynb_checkpoints/*
*.ipynb_checkpoints

# Notebook temporÃ¡rios
.jupyter/
.local/

# ============================================
# Dados (NÃƒO commitar dados grandes!)
# ============================================
data/raw/*
data/processed/*
!data/raw/.gitkeep
!data/processed/.gitkeep

# Datasets
*.csv
*.parquet
*.avro
*.orc
*.json
*.xml
*.xlsx
*.xls

# Exceto exemplos pequenos
!data/seeds/*.csv
!data/examples/*.json

# ============================================
# Modelos ML (arquivos grandes)
# ============================================
*.pkl
*.joblib
*.h5
*.hdf5
*.pb
*.ckpt
*.pth
*.pt
*.safetensors

# Exceto modelos pequenos ou metadados
!models/*/metadata.json
!models/*/metrics.json

# ============================================
# Logs e Outputs
# ============================================
logs/
*.log
*.out
*.err

# Airflow
airflow/logs/
airflow.db
airflow.cfg
webserver_config.py
unittests.cfg

# dbt
dbt/logs/
dbt/target/
dbt/dbt_packages/

# Spark
spark-warehouse/
metastore_db/
derby.log

# ============================================
# Credenciais e Secrets (NUNCA commitar!)
# ============================================
.env
.env.*
!.env.example
*.key
*.pem
*.crt
secrets/
credentials/
*.credentials
*secret*
*password*
*token*

# Cloud provider credentials
.aws/
.gcp/
.azure/

# Database connection strings
connection_strings.txt
db_credentials.json

# ============================================
# IDEs e Editores
# ============================================
# VS Code
.vscode/
*.code-workspace

# PyCharm
.idea/
*.iml

# Jupyter
.ipynb_checkpoints/

# Vim
*.swp
*.swo
*~

# Emacs
*~
\#*\#

# ============================================
# Sistema Operacional
# ============================================
# macOS
.DS_Store
.AppleDouble
.LSOverride

# Windows
Thumbs.db
desktop.ini
$RECYCLE.BIN/

# Linux
.directory

# ============================================
# Docker
# ============================================
.dockerignore

# ============================================
# TemporÃ¡rios e Cache
# ============================================
tmp/
temp/
cache/
.cache/
*.tmp
*.temp
*.bak
*.swp

# ============================================
# Build e Compilados
# ============================================
*.class
*.o
*.so
*.dylib

# ============================================
# Testes
# ============================================
.tox/
.coverage
.coverage.*
htmlcov/
.pytest_cache/

# ============================================
# EspecÃ­ficos de Ferramentas
# ============================================

# Terraform
.terraform/
*.tfstate
*.tfstate.backup

# Spark
spark-warehouse/
derby.log
metastore_db/

# Kafka
/logs/

# Prefect
.prefect/

# Great Expectations
uncommitted/
```

### .gitkeep para Pastas Vazias

Adicione arquivo `.gitkeep` em pastas que precisam existir mas comeÃ§am vazias:

```bash
data/raw/.gitkeep
data/processed/.gitkeep
logs/.gitkeep
models/.gitkeep
```

---

## ğŸ“š DocumentaÃ§Ã£o ObrigatÃ³ria {#documentaÃ§Ã£o}

### 1. README.md (Raiz do Projeto)

```markdown
# ğŸ“Š Nome do Projeto

[![CI](https://github.com/org/proj/workflows/CI/badge.svg)](https://github.com/org/proj/actions)
[![Coverage](https://codecov.io/gh/org/proj/branch/main/graph/badge.svg)](https://codecov.io/gh/org/proj)

## ğŸ¯ Objetivo
Pipeline de dados que processa vendas diÃ¡rias...

## ğŸ—ï¸ Arquitetura

\`\`\`
[Diagrama Mermaid ou imagem]
\`\`\`

## ğŸ“‹ PrÃ©-requisitos
- Python 3.9+
- PostgreSQL 13+
- Docker & Docker Compose

## ğŸš€ InstalaÃ§Ã£o

\`\`\`bash
# Clone
git clone repo-url

# Setup
make setup

# Teste
make test
\`\`\`

## ğŸ“Š Pipelines

### Pipeline Vendas DiÃ¡rio
- **FrequÃªncia**: DiÃ¡rio Ã s 6h
- **Fonte**: Salesforce API
- **Destino**: Data Warehouse (tabela fct_sales)
- **SLA**: 2 horas
- **Owner**: JoÃ£o Silva (@joao)

## ğŸ—‚ï¸ Estrutura de Dados

Ver [Data Catalog](docs/data_catalog/README.md)

## ğŸ§ª Testes

\`\`\`bash
make test
make test-integration
make test-data-quality
\`\`\`

## ğŸš¢ Deploy

Ver [DEPLOY.md](docs/DEPLOY.md)

## ğŸ“– DocumentaÃ§Ã£o

- [Arquitetura](docs/architecture/README.md)
- [Runbooks](docs/runbooks/README.md)
- [Data Catalog](docs/data_catalog/README.md)

## ğŸ†˜ Suporte

- **On-call**: @data-eng-team
- **Slack**: #data-engineering
- **Email**: data-eng@company.com

## ğŸ‘¥ Time

| Papel | Nome | GitHub |
|-------|------|--------|
| Tech Lead | JoÃ£o Silva | @joao |
| Data Engineer | Maria Santos | @maria |
| Data Analyst | Pedro Costa | @pedro |

## ğŸ“œ LicenÃ§a
MIT
```

### 2. Data Catalog (docs/data_catalog/)

```markdown
# ğŸ“Š Data Catalog

## Tabelas

### dim_clients
**DescriÃ§Ã£o**: DimensÃ£o de clientes

**Schema**:
| Coluna | Tipo | DescriÃ§Ã£o | Exemplo |
|--------|------|-----------|---------|
| client_id | INTEGER | ID Ãºnico | 12345 |
| client_name | VARCHAR(100) | Nome completo | "JoÃ£o Silva" |
| email | VARCHAR(100) | Email | "joao@email.com" |
| created_at | TIMESTAMP | Data criaÃ§Ã£o | 2025-01-01 10:00:00 |
| is_active | BOOLEAN | Cliente ativo | true |

**Fonte**: Salesforce API
**AtualizaÃ§Ã£o**: DiÃ¡ria (6h)
**Owner**: @joao
**SLA**: 2 horas
**RetenÃ§Ã£o**: 5 anos

**Relacionamentos**:
- fct_sales.client_id â†’ dim_clients.client_id
- fct_orders.client_id â†’ dim_clients.client_id

**Queries Comuns**:
\`\`\`sql
-- Clientes ativos
SELECT * FROM dim_clients WHERE is_active = true;

-- Novos clientes no mÃªs
SELECT * FROM dim_clients 
WHERE created_at >= DATE_TRUNC('month', CURRENT_DATE);
\`\`\`
```

### 3. Runbook (docs/runbooks/)

```markdown
# ğŸš¨ Runbook: Pipeline Vendas DiÃ¡rio

## InformaÃ§Ãµes BÃ¡sicas
- **Nome**: pipeline_vendas_diario
- **FrequÃªncia**: DiÃ¡rio Ã s 6h
- **SLA**: 2 horas
- **Owner**: JoÃ£o Silva (@joao)
- **On-call**: Data Engineering Team

## Monitoramento
- **Dashboard**: [Grafana Dashboard](url)
- **Alertas**: Slack #data-alerts
- **Logs**: DataDog

## Troubleshooting

### âŒ Pipeline Falhou
**Sintoma**: Alerta "Pipeline Failed"

**DiagnÃ³stico**:
1. Verificar logs no Airflow
2. Verificar conectividade com fonte
3. Verificar espaÃ§o em disco

**ResoluÃ§Ã£o**:
\`\`\`bash
# 1. Ver logs
docker logs airflow-scheduler

# 2. Reprocessar data especÃ­fica
airflow dags trigger pipeline_vendas_diario \
  --conf '{"date": "2026-01-13"}'
\`\`\`

### âš ï¸ Dados Atrasados
**Sintoma**: Dados nÃ£o atualizados

**DiagnÃ³stico**:
1. Verificar se pipeline rodou
2. Verificar horÃ¡rio da fonte
3. Verificar timezone

**ResoluÃ§Ã£o**:
\`\`\`sql
-- Verificar Ãºltima atualizaÃ§Ã£o
SELECT MAX(updated_at) FROM fct_sales;

-- Reprocessar se necessÃ¡rio
-- [comandos]
\`\`\`

## Rollback

\`\`\`bash
# Voltar para versÃ£o anterior
git checkout v2.0.0
make deploy-staging
# Validar
make deploy-production
\`\`\`

## Contatos
- **Tech Lead**: JoÃ£o Silva (@joao)
- **On-call**: @data-eng-team
- **Escalation**: CTO
```

### 4. ADR - Architecture Decision Records

```markdown
# ADR 001: Escolha de dbt para TransformaÃ§Ãµes

## Status
Aceito

## Contexto
PrecisÃ¡vamos escolher ferramenta para transformaÃ§Ãµes SQL.

OpÃ§Ãµes avaliadas:
- dbt
- Stored Procedures
- Airflow + SQL

## DecisÃ£o
Usar dbt para todas as transformaÃ§Ãµes SQL.

## ConsequÃªncias

### Positivas
- Versionamento de transformaÃ§Ãµes
- Testes de dados integrados
- DocumentaÃ§Ã£o automÃ¡tica
- Lineage de dados
- Reusabilidade (macros)

### Negativas
- Curva de aprendizado
- Necessita setup adicional
- Dependency management

## Alternativas Consideradas
1. **Stored Procedures**: Menos versionamento
2. **Airflow + SQL**: Menos estruturado

## ReferÃªncias
- [dbt Docs](https://docs.getdbt.com)
- [Benchmark interno](link)
```

---

## âœ… Code Review e Qualidade {#code-review}

### Checklist de Pull Request

```markdown
## ğŸ“‹ Checklist

### CÃ³digo
- [ ] Segue padrÃµes de nomenclatura
- [ ] CÃ³digo limpo e legÃ­vel
- [ ] Sem cÃ³digo comentado/morto
- [ ] Sem credenciais hardcoded
- [ ] Tratamento de erros adequado
- [ ] Logging apropriado

### Testes
- [ ] Testes unitÃ¡rios incluÃ­dos
- [ ] Testes de integraÃ§Ã£o (se aplicÃ¡vel)
- [ ] Testes de qualidade de dados
- [ ] Cobertura > 80%
- [ ] Todos os testes passando

### Dados
- [ ] Schema documentado
- [ ] ValidaÃ§Ãµes de qualidade
- [ ] Impacto em downstream documentado
- [ ] Backfill plan (se necessÃ¡rio)

### DocumentaÃ§Ã£o
- [ ] README atualizado
- [ ] Data catalog atualizado
- [ ] Runbook criado/atualizado
- [ ] CHANGELOG atualizado
- [ ] ComentÃ¡rios em cÃ³digo complexo

### Performance
- [ ] Queries otimizadas
- [ ] Ãndices apropriados
- [ ] Particionamento considerado
- [ ] Volume de dados testado

### SeguranÃ§a
- [ ] Sem dados sensÃ­veis expostos
- [ ] Credenciais em secrets
- [ ] Acesso por roles/permissions
- [ ] PII tratado adequadamente

### CI/CD
- [ ] CI passa
- [ ] Linting passa
- [ ] Type checking passa (se Python)
- [ ] Build Docker funciona
```

### Template de Pull Request

```markdown
## ğŸ“ DescriÃ§Ã£o
[DescriÃ§Ã£o clara do que foi mudado e por quÃª]

## ğŸ¯ Tipo de MudanÃ§a
- [ ] Feature (nova funcionalidade)
- [ ] Bugfix (correÃ§Ã£o)
- [ ] Hotfix (correÃ§Ã£o urgente)
- [ ] Refactor (refatoraÃ§Ã£o)
- [ ] Data (mudanÃ§a em dados/schema)
- [ ] Docs (documentaÃ§Ã£o)
- [ ] CI/CD

## ğŸ”— Links
- Jira: DAT-123
- Design Doc: [link]
- Dashboard: [link]

## ğŸ§ª Como Testar
\`\`\`bash
# Passos para testar
make test
make run-local
\`\`\`

## ğŸ“Š Impacto
### Dados
- Tabelas afetadas: `fct_sales`, `dim_clients`
- Breaking changes: NÃ£o
- Backfill necessÃ¡rio: Sim (Ãºltimos 30 dias)

### Downstream
- Dashboards: Dashboard Executivo precisa atualizaÃ§Ã£o
- APIs: Nenhum impacto
- Reports: Report mensal precisa atualizaÃ§Ã£o

## ğŸ“¸ Screenshots
[Se aplicÃ¡vel]

## âœ… Checklist
[Copiar checklist acima]

## ğŸ‘¥ Reviewers
@joao @maria
```

### Code Review Guidelines

#### Para Reviewers:

```markdown
## O que revisar:

### 1. Funcionalidade
âœ… O cÃ³digo faz o que deveria?
âœ… Edge cases cobertos?
âœ… Tratamento de erros adequado?

### 2. Qualidade
âœ… CÃ³digo limpo e legÃ­vel?
âœ… Nomes descritivos?
âœ… Sem duplicaÃ§Ã£o?
âœ… PadrÃµes seguidos?

### 3. Performance
âœ… Queries eficientes?
âœ… Ãndices necessÃ¡rios?
âœ… Pode escalar?

### 4. SeguranÃ§a
âœ… Sem credenciais expostas?
âœ… ValidaÃ§Ã£o de inputs?
âœ… PII protegida?

### 5. Testes
âœ… Testes cobrem mudanÃ§as?
âœ… Testes passam?
âœ… Qualidade de dados validada?

### 6. DocumentaÃ§Ã£o
âœ… CÃ³digo complexo comentado?
âœ… README atualizado?
âœ… Data catalog atualizado?

## Como dar feedback:

### âœ… Bom feedback:
```
"Sugiro usar Ã­ndice em `client_id` pois essa query roda 
em milhÃµes de linhas. Exemplo:
CREATE INDEX idx_sales_client ON fct_sales(client_id);
"
```

### âŒ Feedback ruim:
```
"Isso tÃ¡ ruim"
"NÃ£o funciona"
```

### Tags Ãºteis:
- **[nit]**: Nitpick, opcional
- **[question]**: Pergunta, nÃ£o bloqueante
- **[blocking]**: Deve ser resolvido antes de merge
- **[suggestion]**: SugestÃ£o de melhoria
```

---

## ğŸ”„ Workflows DataOps {#workflows-dataops}

### Workflow 1: Nova Feature

```mermaid
graph LR
    A[Create Branch] --> B[Develop]
    B --> C[Test Locally]
    C --> D[Commit]
    D --> E[Push]
    E --> F[Create PR]
    F --> G[CI/CD]
    G --> H{Passou?}
    H -->|NÃ£o| B
    H -->|Sim| I[Code Review]
    I --> J{Aprovado?}
    J -->|NÃ£o| B
    J -->|Sim| K[Merge to Staging]
    K --> L[Deploy Staging]
    L --> M[Validate]
    M --> N{OK?}
    N -->|NÃ£o| O[Rollback]
    O --> B
    N -->|Sim| P[Merge to Main]
    P --> Q[Deploy Production]
    Q --> R[Monitor]
    R --> S[Delete Branch]
```

#### Comandos:

```bash
# 1. Create branch
git checkout staging
git pull
git checkout -b feature/DAT-123-pipeline-vendas

# 2. Develop
# ... cÃ³digo ...

# 3. Test locally
make test
make test-integration

# 4. Commit
git add .
git commit -m "feat(pipeline): implementa extraÃ§Ã£o de vendas"

# 5. Push
git push origin feature/DAT-123-pipeline-vendas

# 6. Create PR (via GitHub/GitLab UI)

# 7-10. CI/CD e Review automaticamente

# 11. ApÃ³s aprovaÃ§Ã£o e merge
git checkout staging
git pull
# Deploy staging automÃ¡tico

# 12. Validar staging
make validate-staging

# 13. Merge to main (via PR)

# 14. Deploy production (automÃ¡tico)

# 15. Monitor
# Ver Grafana, DataDog, etc

# 16. Cleanup
git checkout main
git pull
git branch -d feature/DAT-123-pipeline-vendas
git push origin --delete feature/DAT-123-pipeline-vendas
```

### Workflow 2: Hotfix

```bash
# 1. Create hotfix branch DIRETO da production
git checkout production
git pull
git checkout -b hotfix/BUG-789-erro-calculo-vendas

# 2. Fix
# ... cÃ³digo ...

# 3. Test
make test

# 4. Commit
git commit -m "fix(vendas): corrige cÃ¡lculo de comissÃ£o"

# 5. Push e PR para production
git push origin hotfix/BUG-789-erro-calculo-vendas
# PR para production

# 6. ApÃ³s aprovaÃ§Ã£o, merge em production
# Deploy automÃ¡tico

# 7. Fazer merge tambÃ©m em staging
git checkout staging
git merge hotfix/BUG-789-erro-calculo-vendas
git push

# 8. Cleanup
git branch -d hotfix/BUG-789-erro-calculo-vendas
```

### Workflow 3: MudanÃ§a de Schema

```bash
# 1. Branch especÃ­fica para dados
git checkout -b data/schema-v2-clientes

# 2. Criar migration
# migrations/002_add_cpf_to_clients.sql
ALTER TABLE dim_clients ADD COLUMN cpf VARCHAR(11);

# 3. Atualizar dbt models
# models/marts/dim_clients.sql
# ... adicionar cpf ...

# 4. Atualizar data catalog
# docs/data_catalog/dim_clients.md
# Documentar nova coluna

# 5. Criar backfill script se necessÃ¡rio
# scripts/migration/backfill_cpf.py

# 6. Testar
make test-data-quality

# 7. Commit
git commit -m "schema(clientes): adiciona campo cpf

- Adiciona coluna cpf em dim_clients
- Atualiza modelo dbt
- Documenta no data catalog
- Inclui script de backfill

BREAKING CHANGE: Schema de dim_clients alterado.
Requer reprocessamento de dados."

# 8. PR com atenÃ§Ã£o especial
# Tag: [BREAKING]
# Reviewers: @tech-lead @data-steward

# 9. Deploy gradual
# Staging primeiro, validar bem
# Depois production
```

### Workflow 4: Experimento/AnÃ¡lise

```bash
# 1. Branch de experimento
git checkout -b experiment/analise-churn-q4

# 2. Desenvolver notebook
# notebooks/experiments/2026_01_13_analise_churn.ipynb

# 3. Commitar resultados (nÃ£o dados!)
git add notebooks/experiments/
git commit -m "experiment: anÃ¡lise de churn Q4 2025

Insights:
- Taxa de churn aumentou 15% em dezembro
- Principais motivos: preÃ§o e concorrÃªncia
- RecomendaÃ§Ãµes: programa de retenÃ§Ã£o

Refs: JIRA-456"

# 4. NÃ£o fazer merge!
# Ou fazer merge apenas se virar feature permanente
# Caso contrÃ¡rio, manter branch ou deletar apÃ³s apresentaÃ§Ã£o

# 5. Se virar feature
git checkout staging
git checkout -b feature/modelo-previsao-churn
# Transformar notebook em pipeline produtivo
# ... desenvolvimento ...
# PR normal
```

---

## ğŸ”’ SeguranÃ§a e Compliance {#seguranÃ§a}

### Secrets Management

#### âŒ NUNCA faÃ§a isso:

```python
# ERRADO!
DATABASE_PASSWORD = "senha123"
API_KEY = "abc123xyz"

# ERRADO!
connection_string = "postgresql://user:password@host/db"
```

#### âœ… FaÃ§a isso:

```python
# CERTO!
import os
from dotenv import load_dotenv

load_dotenv()

DATABASE_PASSWORD = os.getenv("DATABASE_PASSWORD")
API_KEY = os.getenv("API_KEY")
```

```.env
# .env (NÃƒO commitar!)
DATABASE_PASSWORD=senha_segura
API_KEY=chave_api_secreta
```

```bash
# .env.example (SIM commitar!)
DATABASE_PASSWORD=sua_senha_aqui
API_KEY=sua_chave_aqui
```

### Uso de Secrets Managers

```python
# AWS Secrets Manager
import boto3

def get_secret(secret_name):
    client = boto3.client('secretsmanager')
    response = client.get_secret_value(SecretId=secret_name)
    return response['SecretString']

DATABASE_PASSWORD = get_secret('prod/database/password')
```

### PII (Personally Identifiable Information)

#### Identificar PII:

```python
# Dados considerados PII:
- CPF, RG, Passaporte
- Nome completo
- Email
- Telefone
- EndereÃ§o completo
- Data de nascimento
- Dados financeiros (cartÃ£o de crÃ©dito, conta bancÃ¡ria)
- Dados de saÃºde
- Dados biomÃ©tricos
```

#### Tratamento de PII:

```python
# Hash de PII
import hashlib

def hash_pii(value):
    """Hash irreversÃ­vel de PII"""
    return hashlib.sha256(value.encode()).hexdigest()

# Uso
cpf_hash = hash_pii("12345678901")

# Mascaramento
def mask_email(email):
    """Mascara email: jo**@ex***.com"""
    username, domain = email.split('@')
    return f"{username[:2]}**@{domain[:2]}***.com"

# TokenizaÃ§Ã£o (reversÃ­vel com chave)
from cryptography.fernet import Fernet

key = Fernet.generate_key()
cipher = Fernet(key)

def tokenize_pii(value):
    """Tokeniza PII de forma reversÃ­vel"""
    return cipher.encrypt(value.encode())

def detokenize_pii(token):
    """Destokeniza PII"""
    return cipher.decrypt(token).decode()
```

### NÃ­veis de Acesso

```sql
-- Criar roles
CREATE ROLE data_analyst;
CREATE ROLE data_engineer;
CREATE ROLE data_admin;

-- PermissÃµes por camada
-- Bronze/Raw: Apenas engineers
GRANT SELECT ON raw.* TO data_engineer;

-- Silver/Staging: Engineers e alguns analysts
GRANT SELECT ON staging.* TO data_engineer;
GRANT SELECT ON staging.* TO data_analyst;

-- Gold/Marts: Todos
GRANT SELECT ON marts.* TO data_analyst;

-- PII: Apenas admin com justificativa
GRANT SELECT ON sensitive.clients_pii TO data_admin;

-- Views sem PII para anÃ¡lise
CREATE VIEW marts.clients_analytics AS
SELECT
    client_id,
    hash(cpf) as cpf_hash,  -- hash ao invÃ©s de CPF real
    age_group,  -- faixa etÃ¡ria ao invÃ©s de data nascimento
    city  -- cidade ao invÃ©s de endereÃ§o completo
FROM sensitive.clients_pii;

GRANT SELECT ON marts.clients_analytics TO data_analyst;
```

### Auditoria

```sql
-- Log de acessos a dados sensÃ­veis
CREATE TABLE audit.data_access_log (
    id SERIAL PRIMARY KEY,
    user_name VARCHAR(100),
    table_name VARCHAR(100),
    action VARCHAR(50),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    query_text TEXT
);

-- Trigger para log
CREATE TRIGGER log_access
AFTER SELECT ON sensitive.clients_pii
FOR EACH STATEMENT
EXECUTE FUNCTION log_data_access();
```

### LGPD/GDPR Compliance

```python
# Direito ao esquecimento
def delete_user_data(user_id):
    """
    Remove todos os dados de um usuÃ¡rio (LGPD Art. 18)
    """
    # Deletar dados pessoais
    db.execute("""
        DELETE FROM clients WHERE client_id = %s
    """, (user_id,))
    
    # Anonimizar dados histÃ³ricos
    db.execute("""
        UPDATE orders 
        SET client_name = 'ANONYMIZED',
            client_email = 'ANONYMIZED'
        WHERE client_id = %s
    """, (user_id,))
    
    # Log da aÃ§Ã£o
    audit_log(f"User {user_id} data deleted - LGPD request")

# ExportaÃ§Ã£o de dados (LGPD Art. 18)
def export_user_data(user_id):
    """
    Exporta todos os dados de um usuÃ¡rio
    """
    data = {
        'personal_info': get_personal_info(user_id),
        'orders': get_orders(user_id),
        'interactions': get_interactions(user_id)
    }
    return data
```

### Checklist de SeguranÃ§a

```markdown
## ğŸ”’ Security Checklist

### CÃ³digo
- [ ] Sem credenciais hardcoded
- [ ] Secrets em secrets manager
- [ ] .env nÃ£o commitado
- [ ] .env.example commitado

### Dados
- [ ] PII identificado
- [ ] PII protegido (hash/tokenizaÃ§Ã£o)
- [ ] Acesso por roles/permissions
- [ ] Auditoria configurada
- [ ] RetenÃ§Ã£o de dados definida

### Infraestrutura
- [ ] ConexÃµes criptografadas (SSL/TLS)
- [ ] Rede privada (VPC)
- [ ] Firewall configurado
- [ ] Backups criptografados

### Compliance
- [ ] LGPD/GDPR compliance
- [ ] PolÃ­tica de privacidade
- [ ] Consentimento documentado
- [ ] Processo de exclusÃ£o de dados

### Code Review
- [ ] Scan de secrets automÃ¡tico
- [ ] Dependency scan (vulnerabilidades)
- [ ] SAST (Static Analysis)
- [ ] AprovaÃ§Ã£o de security team
```

---

## âœ… Checklist Completo de Projeto {#checklist}

### Checklist de Setup Inicial

```markdown
## ğŸ“‹ Setup de Novo Projeto DataOps

### RepositÃ³rio
- [ ] Criar repositÃ³rio no GitHub/GitLab
- [ ] Adicionar descriÃ§Ã£o clara
- [ ] Configurar .gitignore (use template DataOps)
- [ ] Adicionar LICENSE
- [ ] Configurar branch protection (main/staging)
- [ ] Adicionar PR template
- [ ] Adicionar issue templates

### Estrutura
- [ ] Criar estrutura de pastas padrÃ£o
- [ ] Adicionar .gitkeep em pastas vazias
- [ ] Configurar prÃ©-commit hooks
- [ ] Setup Makefile com comandos Ãºteis

### DocumentaÃ§Ã£o
- [ ] README.md completo
- [ ] CONTRIBUTING.md
- [ ] CHANGELOG.md
- [ ] CODE_OF_CONDUCT.md
- [ ] Docs de arquitetura
- [ ] Data catalog inicial
- [ ] Runbooks de operaÃ§Ã£o

### CI/CD
- [ ] Configurar GitHub Actions / GitLab CI
- [ ] Pipeline de testes
- [ ] Pipeline de deploy (staging)
- [ ] Pipeline de deploy (production)
- [ ] Linting automÃ¡tico
- [ ] Security scanning
- [ ] Dependency checking

### SeguranÃ§a
- [ ] Secrets configurados
- [ ] Roles e permissÃµes
- [ ] Scan de secrets automÃ¡tico
- [ ] .env.example criado
- [ ] Compliance checklist

### Qualidade
- [ ] Testes unitÃ¡rios setup
- [ ] Testes integraÃ§Ã£o setup
- [ ] Data quality tests
- [ ] Code coverage > 80%
- [ ] Linting configurado (black, flake8, mypy)

### Monitoramento
- [ ] Logging configurado
- [ ] MÃ©tricas definidas
- [ ] Alertas configurados
- [ ] Dashboard criado
- [ ] On-call rotation

### Time
- [ ] CODEOWNERS configurado
- [ ] Reviewers definidos
- [ ] On-call definido
- [ ] DocumentaÃ§Ã£o de papÃ©is
```

### Checklist de Pull Request

```markdown
## ğŸ“‹ Antes de Abrir PR

### CÃ³digo
- [ ] CÃ³digo limpo e legÃ­vel
- [ ] Segue padrÃµes do projeto
- [ ] Conventional commits
- [ ] Sem cÃ³digo morto/comentado
- [ ] Tratamento de erros
- [ ] Logging adequado

### Testes
- [ ] Testes unitÃ¡rios escritos
- [ ] Testes passando localmente
- [ ] Coverage mantido/melhorado
- [ ] Testes de integraÃ§Ã£o (se necessÃ¡rio)
- [ ] Data quality tests

### DocumentaÃ§Ã£o
- [ ] CÃ³digo complexo comentado
- [ ] README atualizado
- [ ] Data catalog atualizado
- [ ] CHANGELOG atualizado
- [ ] Runbook atualizado (se necessÃ¡rio)

### Dados
- [ ] Schema documentado
- [ ] ValidaÃ§Ãµes implementadas
- [ ] Impacto analisado
- [ ] Backfill planejado (se necessÃ¡rio)

### SeguranÃ§a
- [ ] Sem secrets expostos
- [ ] PII tratado adequadamente
- [ ] PermissÃµes corretas
- [ ] Scan de seguranÃ§a passou

### Performance
- [ ] Queries otimizadas
- [ ] Ãndices considerados
- [ ] Volume de dados testado
- [ ] Particionamento avaliado

### CI/CD
- [ ] CI passa localmente
- [ ] Build bem-sucedido
- [ ] Linting passa
- [ ] Type checking passa
```

### Checklist de Deploy

```markdown
## ğŸ“‹ Antes de Deploy

### Pre-Deploy
- [ ] PR aprovado por 2+ reviewers
- [ ] CI/CD verde
- [ ] Testes passando (100%)
- [ ] Code coverage > 80%
- [ ] Security scan passou
- [ ] Staging testado e validado

### ComunicaÃ§Ã£o
- [ ] Time notificado
- [ ] Stakeholders informados
- [ ] Janela de manutenÃ§Ã£o (se necessÃ¡rio)
- [ ] Runbook atualizado

### Dados
- [ ] Backfill planejado
- [ ] Migration scripts prontos
- [ ] Rollback plan documentado
- [ ] Data quality validado

### Deploy
- [ ] Backup realizado
- [ ] Deploy executado
- [ ] Smoke tests passaram
- [ ] Monitoramento OK
- [ ] MÃ©tricas normais

### Post-Deploy
- [ ] ValidaÃ§Ã£o em produÃ§Ã£o
- [ ] Dashboard atualizado
- [ ] DocumentaÃ§Ã£o atualizada
- [ ] Time notificado (sucesso)
- [ ] Post-mortem (se problemas)
```

---

## ğŸ“ Resumo e Boas PrÃ¡ticas Gerais

### âœ… SEMPRE FaÃ§a:

1. **Commits frequentes e pequenos**
2. **Mensagens de commit claras** (Conventional Commits)
3. **Pull antes de push**
4. **Code review obrigatÃ³rio**
5. **Testes automatizados**
6. **DocumentaÃ§Ã£o atualizada**
7. **Secrets em secrets manager**
8. **PII protegido**
9. **Branch por feature**
10. **CI/CD funcionando**

### âŒ NUNCA FaÃ§a:

1. **Commitar credenciais**
2. **Commitar dados grandes**
3. **Commit direto em main/production**
4. **Force push em branches compartilhadas**
5. **Deixar cÃ³digo morto**
6. **Mensagens vagas ("fix", "update")**
7. **Skip de testes**
8. **Deploy sem validaÃ§Ã£o**
9. **Ignorar code review**
10. **Esquecer documentaÃ§Ã£o**

### ğŸ“Š MÃ©tricas de Qualidade

Acompanhe estas mÃ©tricas:

```
âœ… Code Coverage: > 80%
âœ… PR Merge Time: < 24h
âœ… CI/CD Success Rate: > 95%
âœ… Mean Time to Recovery: < 1h
âœ… Deployment Frequency: Daily
âœ… Lead Time for Changes: < 1 day
âœ… Change Failure Rate: < 5%
```

---

## ğŸ”— Recursos Adicionais

### Templates e Exemplos

- **GitHub Templates**: [github.com/github/gitignore](https://github.com/github/gitignore)
- **Conventional Commits**: [conventionalcommits.org](https://www.conventionalcommits.org/)
- **Semantic Versioning**: [semver.org](https://semver.org/)

### Ferramentas Recomendadas

- **Pre-commit hooks**: [pre-commit.com](https://pre-commit.com)
- **Commitizen**: Auxilia em conventional commits
- **GitLeaks**: Scan de secrets
- **Black**: Code formatter Python
- **Flake8**: Linting Python
- **MyPy**: Type checking Python

### Leitura Complementar

- **Pro Git Book**: [git-scm.com/book](https://git-scm.com/book)
- **GitHub Flow**: [docs.github.com/en/get-started/quickstart/github-flow](https://docs.github.com/en/get-started/quickstart/github-flow)
- **Data Engineering Best Practices**: Blogs e documentaÃ§Ã£o de Airflow, dbt, etc.

---

**Ãšltima atualizaÃ§Ã£o:** Janeiro 2026

**Mantenedor:** Time de Data Engineering

**VersÃ£o:** 1.0.0
